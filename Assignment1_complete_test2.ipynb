{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "slideshow": {
          "slide_type": "notes"
        }
      },
      "outputs": [],
      "source": [
        "#Updated Strategy\n",
        "#To address the unchanged perplexity, we’ll update the code to:\n",
        "\n",
        "#Remove Stopword Filtering: Increase vocabulary size and context.\n",
        "#Test Stemming Instead of Lemmatization: Reduce vocabulary size more aggressively.\n",
        "#Use Simple split() Tokenization: Match the original code’s approach.\n",
        "#Increase num_merges to 1000: Improve subword coverage.\n",
        "#Fix Validation Tokenization: Ensure subword units are consistent across training and validation.\n",
        "#Add Backoff Smoothing for Bigrams: Improve handling of unseen bigrams.\n",
        "#Enhance Debugging: Print top subword units and their frequencies to verify BPE effectiveness."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /Users/shamitha/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training vocabulary size: 4301\n",
            "Frequency of <unk> in validation set (before subword): 162\n",
            "Frequency of <unk> in training set: 1830\n",
            "Sample subword tokenizations:\n",
            "priceline -> ['pricel', 'in', 'e']\n",
            "travelzoo -> ['trav', 'el', 'z', 'oo']\n",
            "booked -> ['b', 'oo', 'k', 'e', 'd']\n",
            "Top 5 subword units:\n",
            "the: 5292\n",
            "and: 2593\n",
            "a: 2246\n",
            "to: 2090\n",
            "wa: 1826\n",
            "Subword training vocabulary size: 2472\n",
            "Frequency of <unk> in validation set (after subword): 601\n",
            "Perplexity for unigram model (Laplace) on val.txt: 325.44265502170816\n",
            "Perplexity for bigram model (Backoff) on val.txt: 21.358844475560957\n",
            "Perplexity for unigram model (Laplace) with subword on val.txt: 515.9757944669111\n",
            "Perplexity for bigram model (Backoff) with subword on val.txt: 22.13630860092163\n"
          ]
        }
      ],
      "source": [
        "# Assignment1_complete.py\n",
        "\n",
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "from collections import defaultdict, Counter\n",
        "import math\n",
        "\n",
        "# Download NLTK resources (run once)\n",
        "nltk.download('punkt')\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"\n",
        "    Preprocess the input text by:\n",
        "    1. Lowercasing\n",
        "    2. Tokenization with split()\n",
        "    3. Keeping alphanumeric tokens\n",
        "    4. Stemming with PorterStemmer\n",
        "    \"\"\"\n",
        "    text = text.lower()\n",
        "    tokens = text.strip().split()\n",
        "    tokens = [word for word in tokens if word.isalnum()]\n",
        "    tokens = [stemmer.stem(word) for word in tokens]\n",
        "    return tokens\n",
        "\n",
        "# Preprocessing for training set\n",
        "training = 'train.txt'\n",
        "tokens = []\n",
        "\n",
        "with open(training, 'r', encoding='utf-8') as t:\n",
        "    for line in t:\n",
        "        processed_tokens = preprocess_text(line.strip())\n",
        "        tokens_per_line = ['<s>'] + processed_tokens + ['</s>']\n",
        "        tokens.append(tokens_per_line)\n",
        "\n",
        "# Preprocessing for validation set\n",
        "validation = 'val.txt'\n",
        "validation_tokens = []\n",
        "\n",
        "with open(validation, 'r', encoding='utf-8') as v:\n",
        "    for line in v:\n",
        "        processed_tokens = preprocess_text(line.strip())\n",
        "        tokens_per_line = ['<s>'] + processed_tokens + ['</s>']\n",
        "        validation_tokens.append(tokens_per_line)\n",
        "\n",
        "# Unigram model\n",
        "unigram_counts = {}\n",
        "total_word_count = 0\n",
        "for tokens_per_line in tokens:\n",
        "    for token in tokens_per_line:\n",
        "        if token not in unigram_counts:\n",
        "            unigram_counts[token] = 1\n",
        "        else:\n",
        "            unigram_counts[token] += 1\n",
        "        total_word_count += 1\n",
        "\n",
        "unigram_model = {}\n",
        "for word, count in unigram_counts.items():\n",
        "    unigram_model[word] = count / total_word_count\n",
        "\n",
        "unsmoothened_unigram_model = unigram_model.copy()\n",
        "\n",
        "# Debugging: Print vocabulary size\n",
        "print(f\"Training vocabulary size: {len(unigram_counts)}\")\n",
        "\n",
        "# Debugging: Validation <unk> frequency\n",
        "unk_count_val = sum(1 for line in validation_tokens for token in line if token not in unigram_counts and token not in ['<s>', '</s>'])\n",
        "print(f\"Frequency of <unk> in validation set (before subword): {unk_count_val}\")\n",
        "\n",
        "# Bigram model with fix for KeyError\n",
        "bigram_counts = {}\n",
        "for tokens_per_line in tokens:\n",
        "    for i in range(len(tokens_per_line) - 1):\n",
        "        bigram = (tokens_per_line[i], tokens_per_line[i + 1])\n",
        "        if bigram not in bigram_counts:\n",
        "            bigram_counts[bigram] = 1\n",
        "        else:\n",
        "            bigram_counts[bigram] += 1\n",
        "\n",
        "bigram_model = {}\n",
        "for bigram, count in bigram_counts.items():\n",
        "    first_word = bigram[0]\n",
        "    if first_word in unigram_counts:\n",
        "        bigram_model[bigram] = count / unigram_counts[first_word]\n",
        "    else:\n",
        "        print(f\"Warning: Skipping bigram {bigram} because '{first_word}' not found in unigram_counts\")\n",
        "\n",
        "unsmoothened_bigram_model = bigram_model.copy()\n",
        "\n",
        "# First method to handle unknown words\n",
        "common_words = {}\n",
        "for word, count in unigram_counts.items():\n",
        "    if count > 1:\n",
        "        common_words[word] = count\n",
        "\n",
        "tokens_with_unk = tokens.copy()\n",
        "for tokens_per_line in tokens_with_unk:\n",
        "    for i in range(len(tokens_per_line)):\n",
        "        if tokens_per_line[i] not in common_words:\n",
        "            tokens_per_line[i] = '<unk>'\n",
        "\n",
        "# Unigram model with <unk>\n",
        "unigram_counts_with_unk = {}\n",
        "total_word_count_with_unk = 0\n",
        "for tokens_per_line in tokens_with_unk:\n",
        "    for token in tokens_per_line:\n",
        "        if token not in unigram_counts_with_unk:\n",
        "            unigram_counts_with_unk[token] = 1\n",
        "        else:\n",
        "            unigram_counts_with_unk[token] += 1\n",
        "        total_word_count_with_unk += 1\n",
        "\n",
        "unigram_model = {}\n",
        "for unigram, count in unigram_counts_with_unk.items():\n",
        "    unigram_model[unigram] = count / total_word_count_with_unk\n",
        "\n",
        "unigram_model_with_unk = unigram_model.copy()\n",
        "\n",
        "# Debugging: Print <unk> frequency\n",
        "print(f\"Frequency of <unk> in training set: {unigram_counts_with_unk.get('<unk>', 0)}\")\n",
        "\n",
        "# Bigram model with <unk>\n",
        "bigram_counts_with_unk = {}\n",
        "for tokens_per_line in tokens_with_unk:\n",
        "    for i in range(len(tokens_per_line) - 1):\n",
        "        bigram = (tokens_per_line[i], tokens_per_line[i + 1])\n",
        "        if bigram not in bigram_counts_with_unk:\n",
        "            bigram_counts_with_unk[bigram] = 1\n",
        "        else:\n",
        "            bigram_counts_with_unk[bigram] += 1\n",
        "\n",
        "bigram_model = {}\n",
        "for bigram, count in bigram_counts_with_unk.items():\n",
        "    first_word = bigram[0]\n",
        "    bigram_model[bigram] = count / unigram_counts_with_unk[first_word]\n",
        "\n",
        "bigram_model_with_unk = bigram_model.copy()\n",
        "\n",
        "# Second unknown words handling: Subword tokenization (BPE)\n",
        "def get_stats(vocab):\n",
        "    pairs = defaultdict(int)\n",
        "    for word, freq in vocab.items():\n",
        "        symbols = word.split()\n",
        "        for i in range(len(symbols)-1):\n",
        "            pairs[symbols[i], symbols[i+1]] += freq\n",
        "    return pairs\n",
        "\n",
        "def merge_vocab(pair, v_in):\n",
        "    v_out = {}\n",
        "    bigram = re.escape(' '.join(pair))\n",
        "    p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
        "    for word in v_in:\n",
        "        w_out = p.sub(''.join(pair), word)\n",
        "        v_out[w_out] = v_in[word]\n",
        "    return v_out\n",
        "\n",
        "vocab = defaultdict(int)\n",
        "for line in tokens:\n",
        "    for token in line[1:-1]:\n",
        "        formatted = ' '.join(list(token)) + ' </w>'\n",
        "        vocab[formatted] += 1\n",
        "\n",
        "num_merges = 1000  # Increased from 500\n",
        "merges = {}\n",
        "for i in range(num_merges):\n",
        "    pairs = get_stats(vocab)\n",
        "    if not pairs:\n",
        "        break\n",
        "    best = max(pairs, key=pairs.get)\n",
        "    merges[best] = i\n",
        "    vocab = merge_vocab(best, vocab)\n",
        "\n",
        "def tokenize(word, merges):\n",
        "    symbols = list(word)\n",
        "    while True:\n",
        "        pair = None\n",
        "        min_rank = float('inf')\n",
        "        idx = -1\n",
        "        for i in range(len(symbols) - 1):\n",
        "            p = (symbols[i], symbols[i+1])\n",
        "            if p in merges:\n",
        "                rank = merges[p]\n",
        "                if rank < min_rank:\n",
        "                    min_rank = rank\n",
        "                    pair = p\n",
        "                    idx = i\n",
        "        if pair is None:\n",
        "            break\n",
        "        new_symbol = pair[0] + pair[1]\n",
        "        symbols = symbols[:idx] + [new_symbol] + symbols[idx+2:]\n",
        "    return symbols\n",
        "\n",
        "# Debugging: Print sample subword tokenizations and top subwords\n",
        "sample_words = ['priceline', 'travelzoo', 'booked']\n",
        "print(\"Sample subword tokenizations:\")\n",
        "for word in sample_words:\n",
        "    print(f\"{word} -> {tokenize(word, merges)}\")\n",
        "\n",
        "# Apply subword tokenization to training set\n",
        "tokens_with_subword = [line.copy() for line in tokens]\n",
        "for line in tokens_with_subword:\n",
        "    i = 0\n",
        "    while i < len(line):\n",
        "        token = line[i]\n",
        "        if token not in common_words and token not in ['<s>', '</s>']:\n",
        "            subwords = tokenize(token, merges)\n",
        "            line[i:i+1] = subwords\n",
        "            i += len(subwords)\n",
        "        else:\n",
        "            i += 1\n",
        "\n",
        "# Unigram model with subword\n",
        "unigram_counts_with_subword = {}\n",
        "total_word_count_with_subword = 0\n",
        "for tokens_per_line in tokens_with_subword:\n",
        "    for token in tokens_per_line:\n",
        "        if token not in unigram_counts_with_subword:\n",
        "            unigram_counts_with_subword[token] = 1\n",
        "        else:\n",
        "            unigram_counts_with_subword[token] += 1\n",
        "        total_word_count_with_subword += 1\n",
        "\n",
        "# Add <unk> to subword vocabulary\n",
        "unigram_counts_with_subword['<unk>'] = 1\n",
        "total_word_count_with_subword += 1\n",
        "\n",
        "unigram_model_subword = {}\n",
        "for word, count in unigram_counts_with_subword.items():\n",
        "    unigram_model_subword[word] = count / total_word_count_with_subword\n",
        "\n",
        "# Debugging: Print top 5 subword units\n",
        "print(\"Top 5 subword units:\")\n",
        "for word, count in sorted(unigram_counts_with_subword.items(), key=lambda x: x[1], reverse=True)[:5]:\n",
        "    print(f\"{word}: {count}\")\n",
        "\n",
        "# Bigram model with subword\n",
        "bigram_counts_with_subword = {}\n",
        "for tokens_per_line in tokens_with_subword:\n",
        "    for i in range(len(tokens_per_line)-1):\n",
        "        bigram = (tokens_per_line[i], tokens_per_line[i+1])\n",
        "        if bigram not in bigram_counts_with_subword:\n",
        "            bigram_counts_with_subword[bigram] = 1\n",
        "        else:\n",
        "            bigram_counts_with_subword[bigram] += 1\n",
        "\n",
        "bigram_model_with_subword = {}\n",
        "for bigram, count in bigram_counts_with_subword.items():\n",
        "    first_word = bigram[0]\n",
        "    if first_word in unigram_counts_with_subword:\n",
        "        bigram_model_with_subword[bigram] = count / unigram_counts_with_subword[first_word]\n",
        "    else:\n",
        "        print(f\"Warning: Skipping bigram {bigram} because '{first_word}' not found in unigram_counts_with_subword\")\n",
        "\n",
        "# Apply subword tokenization to validation set\n",
        "validation_tokens_with_subword = [line.copy() for line in validation_tokens]\n",
        "for line in validation_tokens_with_subword:\n",
        "    i = 0\n",
        "    while i < len(line):\n",
        "        token = line[i]\n",
        "        if token not in common_words and token not in ['<s>', '</s>']:\n",
        "            subwords = tokenize(token, merges)\n",
        "            line[i:i+1] = subwords\n",
        "            i += len(subwords)\n",
        "        else:\n",
        "            i += 1\n",
        "\n",
        "# Debugging: Print subword vocabulary size and validation <unk> frequency\n",
        "print(f\"Subword training vocabulary size: {len(unigram_counts_with_subword)}\")\n",
        "unk_count_val_subword = sum(1 for line in validation_tokens_with_subword for token in line if token not in unigram_counts_with_subword and token not in ['<s>', '</s>'])\n",
        "print(f\"Frequency of <unk> in validation set (after subword): {unk_count_val_subword}\")\n",
        "\n",
        "# Laplace smoothing\n",
        "unigram_model = {}\n",
        "for word, count in unigram_counts_with_unk.items():\n",
        "    unigram_model[word] = (count + 1) / (total_word_count_with_unk + len(unigram_counts_with_unk))\n",
        "unigram_model_laplace = unigram_model.copy()\n",
        "\n",
        "bigram_model = {}\n",
        "for bigram, count in bigram_counts_with_unk.items():\n",
        "    first_word = bigram[0]\n",
        "    bigram_model[bigram] = (count + 1) / (unigram_counts_with_unk[first_word] + len(unigram_counts_with_unk))\n",
        "bigram_model_laplace = bigram_model.copy()\n",
        "\n",
        "# Laplace smoothing for subword\n",
        "unigram_model_subword = {}\n",
        "for word, count in unigram_counts_with_subword.items():\n",
        "    unigram_model_subword[word] = (count + 1) / (total_word_count_with_subword + len(unigram_counts_with_subword))\n",
        "unigram_model_laplace_subword = unigram_model_subword.copy()\n",
        "\n",
        "bigram_model_subword = {}\n",
        "for bigram, count in bigram_counts_with_subword.items():\n",
        "    first_word = bigram[0]\n",
        "    bigram_model_subword[bigram] = (count + 1) / (unigram_counts_with_subword[first_word] + len(unigram_counts_with_subword))\n",
        "bigram_model_laplace_subword = bigram_model_subword.copy()\n",
        "\n",
        "# Backoff smoothing for bigram\n",
        "def bigram_backoff_perplexity_calculator(validation_data, bigram_model, unigram_model, vocabulary):\n",
        "    V = len(set(vocabulary.keys()))\n",
        "    log_prob_sum = 0.0\n",
        "    N = 0\n",
        "    alpha = 0.4  # Backoff weight\n",
        "    for review in validation_data:\n",
        "        review = [word if word in vocabulary else '<unk>' for word in review]\n",
        "        for i in range(len(review) - 1):\n",
        "            bigram = (review[i], review[i + 1])\n",
        "            if bigram in bigram_model:\n",
        "                probability = bigram_model[bigram]\n",
        "            else:\n",
        "                probability = alpha * unigram_model.get(review[i + 1], 1 / (total_word_count + V))\n",
        "            N += 1\n",
        "            log_prob_sum += math.log(probability)\n",
        "        N += len(review) - 1\n",
        "    if N == 0:\n",
        "        return float('inf')\n",
        "    perplexity = math.exp(-log_prob_sum / N)\n",
        "    return perplexity\n",
        "\n",
        "# Perplexity calculation for unigram model\n",
        "def perplexity_calculator(validation_data, model, vocabulary):\n",
        "    log_prob_sum = 0.0\n",
        "    N = 0\n",
        "    for review_tokens in validation_data:\n",
        "        for token in review_tokens:\n",
        "            if token == '<s>':\n",
        "                continue\n",
        "            N += 1\n",
        "            lookup_token = token\n",
        "            if lookup_token not in vocabulary:\n",
        "                lookup_token = '<unk>'\n",
        "            probability = model.get(lookup_token, 1 / (total_word_count + len(vocabulary)))\n",
        "            log_prob_sum += math.log(probability)\n",
        "    avg_log_prob = log_prob_sum / N\n",
        "    perplexity = math.exp(-avg_log_prob)\n",
        "    return perplexity\n",
        "\n",
        "# Print perplexity results\n",
        "print(f\"Perplexity for unigram model (Laplace) on {validation}: {perplexity_calculator(validation_tokens, unigram_model_laplace, unigram_counts_with_unk)}\")\n",
        "print(f\"Perplexity for bigram model (Backoff) on {validation}: {bigram_backoff_perplexity_calculator(validation_tokens, bigram_model_laplace, unigram_model_laplace, unigram_counts_with_unk)}\")\n",
        "print(f\"Perplexity for unigram model (Laplace) with subword on {validation}: {perplexity_calculator(validation_tokens_with_subword, unigram_model_laplace_subword, unigram_counts_with_subword)}\")\n",
        "print(f\"Perplexity for bigram model (Backoff) with subword on {validation}: {bigram_backoff_perplexity_calculator(validation_tokens_with_subword, bigram_model_laplace_subword, unigram_model_laplace_subword, unigram_counts_with_subword)}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
