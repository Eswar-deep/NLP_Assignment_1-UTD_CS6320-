{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "bTfxMhye-3sb"
      },
      "outputs": [],
      "source": [
        "training='train.txt'\n",
        "#training set tokens\n",
        "tokens=[]\n",
        "\n",
        "with open(training,'r',encoding='utf-8') as t:\n",
        "  for line in t:\n",
        "    each_line_tokens=line.lower().strip().split()\n",
        "    tokens_per_line=['<s>']+ each_line_tokens+['</s>']\n",
        "    tokens.append(tokens_per_line)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "X5aEsceLWDp0"
      },
      "outputs": [],
      "source": [
        "validation='val.txt'\n",
        "# test set tokens\n",
        "validation_tokens=[]\n",
        "\n",
        "with open(validation,'r',encoding='utf-8') as v:\n",
        "  for line in v:\n",
        "    each_line_tokens=line.lower().strip().split()\n",
        "    tokens_per_line=['<s>']+ each_line_tokens+['</s>']\n",
        "    validation_tokens.append(tokens_per_line)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "VaLVmnHrF1qN"
      },
      "outputs": [],
      "source": [
        "unigram_counts={}\n",
        "total_word_count=0\n",
        "# unigram model\n",
        "for tokens_per_line in tokens:\n",
        "  for token in tokens_per_line:\n",
        "    if token not in unigram_counts:\n",
        "      unigram_counts[token]=1\n",
        "    else:\n",
        "      unigram_counts[token]+=1\n",
        "    total_word_count+=1\n",
        "\n",
        "unigram_model={}\n",
        "for word, count in unigram_counts.items():\n",
        "  unigram_model[word]=count/total_word_count\n",
        "\n",
        "unsmoothened_unigram_model=unigram_model.copy()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "YOS7yW1XISpv"
      },
      "outputs": [],
      "source": [
        "bigram_counts={}\n",
        "#bigram_model\n",
        "for tokens_per_line in tokens:\n",
        "  for i in range(len(tokens_per_line)-1):\n",
        "    bigram=(tokens_per_line[i],tokens_per_line[i+1])\n",
        "    if bigram not in bigram_counts:\n",
        "      bigram_counts[bigram]=1\n",
        "    else:\n",
        "      bigram_counts[bigram]+=1\n",
        "\n",
        "\n",
        "bigram_model={}\n",
        "for bigram, count in bigram_counts.items():\n",
        "  first_word=bigram[0]\n",
        "  bigram_model[bigram]=count/unigram_counts[first_word]\n",
        "\n",
        "unsmoothened_bigram_model=bigram_model.copy()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "giMLBF8cUcQC"
      },
      "outputs": [],
      "source": [
        "# first method to handle unkown words\n",
        "common_words={}\n",
        "\n",
        "for word, count in unigram_counts.items():\n",
        "  if count>1:\n",
        "    common_words[word]=count\n",
        "\n",
        "tokens_with_unk=tokens.copy()\n",
        "\n",
        "for tokens_per_line in tokens_with_unk:\n",
        "  for i in range(len(tokens_per_line)):\n",
        "    if tokens_per_line[i] not in common_words:\n",
        "      tokens_per_line[i]='<unk>'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-s8ilhjfjn_H"
      },
      "outputs": [],
      "source": [
        "unigram_counts_with_unk={}\n",
        "total_word_count_with_unk=0\n",
        "for tokens_per_line in tokens_with_unk:\n",
        "  for token in tokens_per_line:\n",
        "    if token not in unigram_counts_with_unk:\n",
        "      unigram_counts_with_unk[token]=1\n",
        "    else:\n",
        "      unigram_counts_with_unk[token]+=1\n",
        "    total_word_count_with_unk+=1\n",
        "\n",
        "unigram_model={}\n",
        "for unigram, count in unigram_counts_with_unk.items():\n",
        "  unigram_model[unigram]=count/total_word_count_with_unk\n",
        "\n",
        "unigram_model_with_unk=unigram_model.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lp2vDSmRkf2n"
      },
      "outputs": [],
      "source": [
        "bigram_counts_with_unk={}\n",
        "for tokens_per_line in tokens_with_unk:\n",
        "  for i in range(len(tokens_per_line)-1):\n",
        "    bigram=(tokens_per_line[i],tokens_per_line[i+1])\n",
        "    if bigram not in bigram_counts_with_unk:\n",
        "      bigram_counts_with_unk[bigram]=1\n",
        "    else:\n",
        "      bigram_counts_with_unk[bigram]+=1\n",
        "\n",
        "bigram_model={}\n",
        "for bigram, count in bigram_counts_with_unk.items():\n",
        "  first_word=bigram[0]\n",
        "  bigram_model[bigram]=count/unigram_counts_with_unk[first_word]\n",
        "\n",
        "bigram_model_with_unk=bigram_model.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1qsXWHsHnB6K"
      },
      "outputs": [],
      "source": [
        "#second unknown words handling subword tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TK_vbJZsq1Il"
      },
      "outputs": [],
      "source": [
        "#Laplace smoothing\n",
        "unigram_model={}\n",
        "for word, count in unigram_counts_with_unk.items():\n",
        "  unigram_model[word]=(count+1)/(total_word_count_with_unk+len(unigram_counts_with_unk))\n",
        "  unigram_model_laplace=unigram_model.copy()\n",
        "\n",
        "bigram_model={}\n",
        "for bigram, count in bigram_counts_with_unk.items():\n",
        "  first_word=bigram[0]\n",
        "  bigram_model[bigram]=(count+1)/(unigram_counts_with_unk[first_word]+len(unigram_counts_with_unk))\n",
        "  bigram_model_laplace=bigram_model.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uneVs722yXZm"
      },
      "outputs": [],
      "source": [
        "#K-smoothing\n",
        "\n",
        "unigram_model={}\n",
        "k=0\n",
        "for word, count in unigram_counts_with_unk.items():\n",
        "  unigram_model[word]=(count+k)/(total_word_count_with_unk+k*len(unigram_counts_with_unk))\n",
        "  unigram_model_laplace_k=unigram_model.copy()\n",
        "\n",
        "bigram_model={}\n",
        "for bigram, count in bigram_counts_with_unk.items():\n",
        "  first_word=bigram[0]\n",
        "  bigram_model[bigram]=(count+k)/(unigram_counts_with_unk[first_word]+k*len(unigram_counts_with_unk))\n",
        "  bigram_model_laplace_k=bigram_model.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eHVqtA561yJ4"
      },
      "outputs": [],
      "source": [
        "#perplexity_unigram_model\n",
        "\n",
        "import math\n",
        "\n",
        "def perplexity_calculator(validation_data, model,vocabulary):\n",
        "  log_prob_sum=0.0\n",
        "  N=0\n",
        "  for review_tokens in validation_data:\n",
        "    for token in review_tokens:\n",
        "      if token=='<s>':\n",
        "        continue\n",
        "\n",
        "      N+=1\n",
        "\n",
        "      lookup_token=token\n",
        "      if lookup_token not in vocabulary:\n",
        "        lookup_token='<unk>'\n",
        "\n",
        "      probablity=model.get(lookup_token,0)\n",
        "\n",
        "      if probablity==0:\n",
        "        print(f\"Error: Unknown word '{token}' found. Perplexity is infinite.\")\n",
        "        return float('inf')\n",
        "\n",
        "      log_prob_sum+=math.log(probablity)\n",
        "\n",
        "  avg_log_prob=log_prob_sum/N\n",
        "  perplexity=math.exp(-avg_log_prob)\n",
        "  return perplexity\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TDjOuPjU8P6a",
        "outputId": "9f2bc7dd-b1c4-4191-b74c-b1fe8d20703c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "293.2282067617357\n"
          ]
        }
      ],
      "source": [
        "print(perplexity_calculator(validation_tokens,unigram_model_laplace_k,unigram_counts_with_unk))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TmOLGYbM4zvq"
      },
      "outputs": [],
      "source": [
        "#perplexity_bigram_model\n",
        "\n",
        "import math\n",
        "\n",
        "def bigram_perplexity_calculator(validation_data, bigram_model, vocabulary):\n",
        "  V=len(set(vocabulary.keys()))\n",
        "  log_prob_sum=0.0\n",
        "  N=0\n",
        "\n",
        "  for review in validation_data:\n",
        "    review = [word if word in vocabulary else '<unk>' for word in review]\n",
        "\n",
        "    for i in range(len(review)-1):\n",
        "      bigram=(review[i],review[i+1])\n",
        "\n",
        "      if(bigram in bigram_model):\n",
        "        probablity=bigram_model[bigram]\n",
        "      else:\n",
        "        probablity=1/(vocabulary.get(review[i],0)+V)\n",
        "\n",
        "\n",
        "      N+=1\n",
        "      log_prob_sum+=math.log(probablity)\n",
        "    N+=len(review)-1\n",
        "\n",
        "  if N==0:\n",
        "    return float('inf')\n",
        "\n",
        "  perplexity = math.exp(-log_prob_sum / N)\n",
        "  return perplexity\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uFXosY-nCVCS",
        "outputId": "4daaf039-a0ad-4160-f215-9c794a89aeb4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "20.714344358932546\n"
          ]
        }
      ],
      "source": [
        "print(bigram_perplexity_calculator(validation_tokens,bigram_model_laplace,unigram_counts_with_unk))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
